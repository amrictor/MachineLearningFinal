{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project: Writing Poetry with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Abigail Rictor, due May 14, 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I decided to explore the idea of writing using a neural network. I am very interested in linguistic structure and how natural language can be broken down and \"understood\" by machines. This has influenced my previous academic and personal projects. Most relevant are a series of Markov algorithms I have written to probabilistically generate different types of text, ranging from tweets to poems. I spent a good deal of last semester working on a Markov based web server which generates poetry based on a 10,000+ element dataset taken from poetryfoundation.org. For this I've developed ways of pre-processing that data to eliminate strange characters as well as methods for post-processing in order to end up with the highest quality content, and that has informed the way I pre- and post- process in this project.\n",
    "\n",
    "Here I am using the same dataset and feeding it to a recurrent neural network which generates new content character by character. A recurrent neural network functions on the principle of remembering state by recycling outputs as inputs. This is useful here because each character we generate relies on the characters generated before it, allowing our output to have some consistency throughout.\n",
    "\n",
    "I was able to generate poems after training on that dataset for different amounts of data, numbers of iterations, and using different batch sizes. Comparing these results to each other gives a more clear idea of the meanings and practical uses of these variables. I also compare methods by looking at these poems side by side with some of the results from my Markov writer with attention to quality, form, and the types of mistakes they make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing external libraries\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from ipywidgets import Dropdown\n",
    "from ipywidgets import Button\n",
    "from IPython.display import display\n",
    "\n",
    "import CharRNN as crnn #See GitHub https://github.com/albertlai431/Machine-Learning/tree/master/Text%20Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a method to read in the text, and maps characters to integers. I am reading in data stored in my local directory which I found by crawling poetryfoundation.org and ripping JSON formatted files for each poem. During this process I replace some unicode characters with ones which will be more recognizable in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(num_poems=5000, all=False):\n",
    "    poems = []\n",
    "    f = IntProgress(min=0, max=num_poems, description= \"Reading data...\")\n",
    "    display(f)\n",
    "    text = \"\"\n",
    "    \n",
    "    poem = random.randint(1,10216)\n",
    "    for i in range(1,num_poems+1):\n",
    "        if(all):\n",
    "            poem = i\n",
    "        else:\n",
    "            poems.append(poem)\n",
    "            while(poem in poems): \n",
    "                poem = random.randint(1, 10215)\n",
    "        text += \"\\n\".join(json.load(open(\"./poems/\"+ str(poem) + \".json\"))['text'])\n",
    "        f.value += 1\n",
    "    f.close()\n",
    "    print(\"Data read complete.\")\n",
    "    \n",
    "    text.replace(u\"\\u0092\", \"'\").replace(\"”\", \"\\\"\").replace(\"“\", \"\\\"\").replace(u\"\\2019\\ufeff\", \"\\'\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I make a call to read the data and then set up two dictionaries which map characters to integers and integers to characters for simple conversion between the characters inputted and outputted and the integers we choose here to represent them in the context of the program. This transforms our data into something potentially mathematically understandable, but isn't enough to define relationships between any given characters. After passing this in for the model to be trained on, the numbers are encoded as one-hot vectors, which allow complex relationships between individual characters to define their own output using vector math.\n",
    "\n",
    "An example of the code to set up and train the recurrent neural network is also below, though I've run it with many other configurations than these. It's calling a file I've formatted and imported which takes much of its code from Shakespeare.py in the github I've listed as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Reading data...', max=5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read complete.\n"
     ]
    }
   ],
   "source": [
    "text = readData(all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Training...', max=492)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Training...', max=984)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Training...', max=1476)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Training...', max=1968)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Training...', max=2460)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Training...', max=2952)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ceff1b321e477082bba4791fc45ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Training...', max=3444)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encoding the text and map each character to an integer and vice versa\n",
    "\n",
    "# We create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# Encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "# Define the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "net = crnn.CharRNN(chars, n_hidden, n_layers)\n",
    "\n",
    "# Declaring the hyperparameters\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 30 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "epochs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 50]\n",
    "\n",
    "for e in epochs:\n",
    "    crnn.train(net, encoded, epochs=e, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=500000)\n",
    "    # Saving the model\n",
    "    model_name = 'rnn_'+str(e)+'_epoch.net'\n",
    "\n",
    "#     checkpoint = {'n_hidden': net.n_hidden,\n",
    "#                   'n_layers': net.n_layers,\n",
    "#                   'state_dict': net.state_dict(),\n",
    "#                   'tokens': net.chars}\n",
    "\n",
    "    with open(model_name, 'wb') as f:\n",
    "        torch.save(net, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The wast'l,!\n",
      "Nor frustes cern,\n",
      "in I redee the frounts the cool unstlien\n",
      "of say a tunce aflears,\n",
      "and with his, deal pingert faping to timntich\n",
      "the wrelised in twher of migher timute ou clamy, of mirllot geats\n",
      "I came the muster ambirt to the clace clurios\n",
      "somerep of a thoussing your, exs stur sostlat\n",
      "That rise oul for the oughing gost smt eid\n",
      "The rost rode?\n",
      "Than brears, in the shosibpos, the best obflounthel.\n",
      "Ald by his means a bore and lin’s bost oad\n",
      "I suld me hed\n",
      "wound so ear youlsay, from sweel,\n",
      "Br\n"
     ]
    }
   ],
   "source": [
    "print(crnn.sample(net, 500, prime=\"The\", top_k=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e3d698f9d046779a2171f4055ad188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Number:', index=1, options=('1', '5', '20'), value='5')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc29045bd5e94d36be6ff19ba174f436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Click me', icon='check', style=ButtonStyle(), tooltip='Click me')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epochs\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-ee1b9f3e2fff>\u001b[0m in \u001b[0;36mload\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rnn_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mdropdown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'_epoch.net'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mload_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mload_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"The\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'eval'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 epochs\n",
      "Their smaty\n",
      "carace torning are I grus\n",
      "Fearte mear on stiff pack ew ase spor its\n",
      "Told refail ham fools in thing\n",
      "To ligount plead ruar of blomip ops,\n",
      "And where panple nowe liped feem by teed\n",
      "wirk rumonared tratl covers,\n",
      "Labe praotirn ent nod begightred my wrombening\n",
      "of ald the cell-the pimes from hear\n",
      "s\n",
      "byom hourses, and aspiront, so ditce is my bus, ap unsueded,\n",
      "Furnt as sasp froores.\n",
      "The bay,\n",
      "but trey lake buruy. So wepel’ins ciudinger of whish\n",
      "bealicly\n",
      "all blew\n",
      "With rynogieves gabfis, a turth, it b\n"
     ]
    }
   ],
   "source": [
    "def load(b):\n",
    "    print(str(dropdown.value) + \" epochs\")\n",
    "    with open('rnn_'+ dropdown.value +'_epoch.net', 'rb') as f:\n",
    "        load_net = torch.load(f)\n",
    "        load_net.eval()\n",
    "        print(crnn.sample(load_net, 500, prime=\"The\", top_k=20))\n",
    "\n",
    "epochs = ['1', '5', '20']\n",
    "dropdown = Dropdown(options=epochs, value='5', description='Number:', disabled=False)\n",
    "button = Button(description='Click me', disabled=False, button_style='success', tooltip='Click me', icon='check')\n",
    "button.on_click(load)\n",
    "display(dropdown, button)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I sample a string of characters from the network by feeding it a \"prime\" which will appear at the beginning of the output and influence what is chosen next. I experimented with some post-processing by iterating through an outputted string and checking for each word (separated by spaces or punctuation) in a dictionary object filled with nearly 500,000 english words. This is loaded from a .json file found on a github listed in the references. I added common contractions to the version of the file found there. When a made-up word is encountered, all text up to that point will be used as the prime in a new sample from the network. This runs until the poem is appropriately long, then cuts off any excess text after the last instance of ending punctuation. This avoids outputting writing which ends in the middle of an idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawSample(prime):\n",
    "    return crnn.sample(net, 500, prime=prime, top_k=20)\n",
    "\n",
    "dictionary = json.load(open(\"./words_dictionary.json\")) #resource to check for real words\n",
    "def checkDictionary(word):\n",
    "    try:\n",
    "        dictionary[word]\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def write(): \n",
    "    poem = crnn.sample(net, 500, prime=\"The\", top_k=20)\n",
    "    poem_arr = poem.split(\" \")\n",
    "    for word in poem_arr: \n",
    "        if not checkDictionary(word.lower()):\n",
    "            poem = crnn.sample(net, 500, prime=poem[:poem.rfind(word)], top_k=20)\n",
    "        if len(poem)>1000:\n",
    "            break\n",
    "    #iterate through words in poem and check if they exist in dictionary\n",
    "    \n",
    "    \n",
    "    last_punctuation = max(poem.rfind('.'), poem.rfind('!'), poem.rfind('?'))\n",
    "    poem = poem[:last_punctuation+1]\n",
    "    print(poem)\n",
    "write()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanFile(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    poem = f.read()\n",
    "    return poem.replace(\"\\n\", \"<br>\")\n",
    "print(\"| Examples |\")\n",
    "epochs = [10, 20]\n",
    "for e in epochs:\n",
    "    poem = cleanFile(\"epoch\"+str(e))\n",
    "    print(\"| 5000 poems, \"+ str(e) +\" epochs |\")\n",
    "    print(\"| <p align=\\\"left\\\">\" + poem +\"</p> |\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps I took.  Resources I used, such as code from the class, on-line resources, research articles, books [Goodfellow, et al., 2016], ....\n",
    "\n",
    "Say in detail what each team member did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Passage from Beowulf, Old English | Poem generated after 1 epoch (5000 poems) |\n",
    "|-----------------------------------|--------------------------------|\n",
    "| <p align=\"left\">Hwaet. We Gardena in geardagum, <br>beodcyninga, brym gefrunon, <br>hu oa aebelingas ellen fremedon. <br>Oft Scyld Scefing sceabena breatum, <br>monegum maegbum, meodosetla ofteah, <br>egsode eorlas. Syooan aerest wearo <br>feasceaft funden, he baes frofre gebad, <br>weox under wolcnum, weoromyndum bah, <br>oobaet him aeghwylc bara ymbsittendra <br>ofer hronrade hyran scolde, <br>gomban gyldan. baet waes god cyning. <br>oaem eafera waes aefter cenned, <br>geong in geardum, bone god sende <br>folce to frofre; fyrenoearfe ongeat <br>be hie aer drugon aldorlease <br>lange hwile. Him baes liffrea, <br>wuldres wealdend, woroldare forgeaf; <br>Beowulf waes breme blaed wide sprang, <br>Scyldes eafera Scedelandum in. </p> | <p align=\"left\">Theis of alr ared ress bit wine nunt epey fotin,<br>wis cand oo thouk ifond risht<br>thel darine, to severy<br>To praarto on.<br>Whe goon woar worl.<br>Whe lanss pirg, inst gon allold,<br>Hode ame heive.<br>Anorecl lint.<br>Denave parlet,<br>Wores, ing dand froake thad a if nifh pad thove made<br>Andry fweang,<br>Trat my sant<br>derred Ind cleen-wars bipd het or tunle, cany<br>we dacl seire thip nattod foy in haln shew<br> I sov a badl rase op the rar lagle, sith es frentes<br>brabs. Yome tea glaoner wet on ninns.<br>Huchey<br>I bocens me goocs<br>in wo</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I experimented by testing different number of epochs when training the network. Because the intention of this program is to generate new content, there aren't specifically accurate values I can aim to achieve. One metric I ended up looking at was the percentage of existing English words used in a given poem. Early in training, the network uses mostly gibberish (which, funnily enough, could probably be mistaken for a poem in old english). By searching for individual words in each poem in a dictionary-dictionary with over 400,000 English words that I loaded from a .JSON file I found online, I was able to quickly assess the number of real and fake words in a string.\n",
    "\n",
    "\n",
    "While setting things up, I often ran the program witha  smaller subset of the data, only 500 poems, and was able to see how few real words it would use with such a small dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 500 poems | 5000 poems |\n",
    "|------------------------|----------------|\n",
    "| 500 poems, 10 epochs | 5000 poems, 10 epochs |\n",
    "|<p align=\"left\">The<br> and that in a chostluce didss in thoughors hipcerias flases  <br>the tulk straaloniss harlery dings lingunt    <br>I having the each me us no could and srid    <br>And can wans bained siclan drod with he tome to but of erond meron whoud,.    <br>Wheon cordow pras righs.,    <br>At thourtandare out four the inistir thar the bets    <br>he sirnored, aln has basing gaib ghay a dimy badn of mepin,    <br>avly about it als bemcore olromlend wa coued as the end.</p> | <p align=\"left\">There who pulses.<br>I askon her world not ripped her;<br>Exwidence so in the longers of death, and chances<br>To all, feared a spilling nouth,<br>Beneath the bad it, at week in the time<br>And you hour the clouds,<br>From the mees singled by peagle down<br>On a ty the sealed of make,<br>Nor dropped shorter with<br>A kip dry legs by blossoms<br>Wherein of shiff yet a squack dirte.<br>The time didn’t said<br>Definive a temption poison.<br>Only happened me so screamed.<br>No morlight he doesn't have finds<br>All that he was sudding it me!<br>Fear t</p> |\n",
    "| 500 poems, 20 epochs | 5000 poems, 20 epochs |\n",
    "| <p align=\"left\">The lingare  <br>Houch batch.  <br>I kune pare,  <br>the forgar,  <br>than it wond tipen, when the glot  <br>Any yeorering laskned,  <br>By creess. Sand.  <br>Saartwas suntery at sernod, on ourland floger barling pillion  <br>of mesing to’th.  <br>I’ve agould hay he.</p> | <p align=\"left\">The woman said then, then<br>is that his heavings and stars<br>in his crowns, within the black cappling,<br>the minerries of it! of their things,<br>who had scan death, is weared<br>they’d been watching the pans and snares.It was a cordipating,<br>All familiar than of the silence of his dark.<br>The name of these muscles trade<br>The satures of leather double-cofse<br>From the teachings of the tack<br>Till he goes blowing to heaven with tood and toss.<br>Now, not I will not see:<br>Through the third grassy windows begin to read the bl</p> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| More Examples using 5000 poems|\n",
    "|----------|\n",
    "| 5000 poems, 3 epochs |\n",
    "| <p align=\"left\">The souls.<br>To swasting, I heap on paber<br>And treeples you parted, is a swear,<br>And the esconderssed time we have gold,<br>Nizely through at the douch from the roses that I<br>while the tall-not hund, cames lade<br>Blank.<br>Still some drive me touches again; to start?<br>I’fr see the great, for a shefred poop to do hap him lafe.All the and bunsmore the firy of my crops<br>O’ceaning two chose-lecken wells and live,<br>And the pood spolls rake cities or city<br>Women her not for a doning telk flower,<br>Sainting skin.<br>The invany </p> |\n",
    "| 5000 poems, 15 epochs |\n",
    "| <p align=\"left\">The wild dream is so is?”<br>She was white, at the great stone?<br>If you understand, except that he loves<br>the shadow in the but song<br>of new hand to crash cornile from table on the white sky,<br>his rings of talle trees, sharpering.<br>Daped I met count through the fields,<br>she said, “It took them.<br>Nearry is all you did not make<br>as the dry deprides. You’re only thought?<br>Some shadow didn’t binst burning death.<br>To think a harm, eyelish is to warm or go in his sweet arms<br>cross personal bodies.I cut a lantup on all </p> |\n",
    "| 5000 poems, 50 epochs |\n",
    "| <p align=\"left\">The hawks were glassed in water,<br>my head still dining.<br>As disclauming he<br>flowed and walking<br>to the price this wile wall<br>because purpose we have to rest.<br>White archivest trocker,<br>the long-sounding remain began standing<br>someone has not speaking gold around<br>his exorvise.<br>Their baby days crowded at a yellow<br>old; accident who can’t finish.<br>There are wings, even with me<br>as if I see in the little ran<br>night wandering like a scattered dish.<br>Shriveling for one land so line—<br>is that trace of a would be already</p> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percents = []\n",
    "epochs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 50]\n",
    "\n",
    "for e in epochs:\n",
    "    f = open(\"epoch\"+str(e), \"r\")\n",
    "    poem = f.read()\n",
    "    poem = poem.replace(\"\\n\", \" \").replace(\".\", \"\").replace(\",\", \"\").replace(\"!\", \"\").replace(\"?\", \"\")\n",
    "    real = 0\n",
    "    poem_list = poem.split(\" \")\n",
    "    for word in poem_list:\n",
    "        if(checkDictionary(word)):\n",
    "            real+=1\n",
    "    percents.append(real/(len(poem_list)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(\"Real Words Generated Across Epochs\")\n",
    "plt.plot(epochs, percents)\n",
    "plt.xticks(epochs, epochs, horizontalalignment='center')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Percent Existing Words')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph can be taken with a grain of salt, because there are of course probabilistic factors affecting the number of real words used every time the network is sampled, but we can notice how as the program is trained, it uses real words more consistently. It learns the rules of the english languages, and even the words that don't exist might look a lot like ones that do, or ones that real people would make up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because my project incorporated a type of network not covered in detail in class, I feel like I learned a lot from this process. Also, since I wasn't working with as familiar code, I spent a lot of time reviewing things that we did cover in class so that I could learn to understand and recognize them in different contexts. \n",
    "\n",
    "Much of the early time spent on this project, I was reformatting code to be more understandable. I added widgets to track training and data reading. I also moved the CharRNN class to its own file, along with some related methods for encoding, training, and sampling. This helped me keep track of the elements I needed to be actively changing, as well as format it in a way that highlights my project above the code I've borrowed from other sources.\n",
    "\n",
    "Some of the difficulties I had with this project include understanding certain pytorch values I'm not familiar with and fully understanding code that I did not write. A lot of machine learning still feels like magic to me, because the math is abstracted from me and it's hard to understand how the output is actually generated.\n",
    "\n",
    "My timeline reflected the one I submitted in my proposal with decent accuracy, though perhaps bumped up by a few days at the beginning because I underestimated my workload upon my return from travelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/albertlai431/Machine-Learning/tree/master/Text%20Generation\n",
    "* https://github.com/dwyl/english-words\n",
    "* https://poetryfoundation.org\n",
    "\n",
    "[Goodfellow, et al., 2016] Ian Goodfellow and Yoshua Bengio and Aaron Courville, [Deep Learning](http://www.deeplearningbook.org), MIT Press. 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Your report for a single person team should contain approximately 2,000 to 5,000 words, in markdown cells.  You can count words by running the following python code in your report directory.  Projects with two people, for example, should contain 4,000 to 8,000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from IPython.nbformat import current\n",
    "import glob\n",
    "nbfile = glob.glob('RictorProjectReport.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
