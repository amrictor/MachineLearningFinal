{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "# from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import json\n",
    "import random\n",
    "import CharRNN as crnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below reads in the text, and maps characters to integers. I am reading in data I found by crawling poetryfoundation.org and ripping JSON formatted files for each poem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def readData(num_poems):\n",
    "    poems = []\n",
    "    f = IntProgress(min=0, max=num_poems, description= \"Reading data...\")\n",
    "    display(f)\n",
    "    text = \"\"\n",
    "    \n",
    "    poem = random.randint(1,10216)\n",
    "    for i in range(1,num_poems+1):\n",
    "        while(poem in poems): \n",
    "            poem = random.randint(1, 10216)\n",
    "        text += \"\\n\".join(json.load(open(\"./poems/\"+ str(poem) + \".json\"))['text'])\n",
    "        f.value += 1\n",
    "    f.close()\n",
    "    print(\"Data read complete.\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Reading data...', max=5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read complete.\n"
     ]
    }
   ],
   "source": [
    "# encoding the text and map each character to an integer and vice versa\n",
    "\n",
    "# We create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to integers\n",
    "text = readData(5000)\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# Encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n",
      "CharRNN(\n",
      "  (lstm): LSTM(186, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=186, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cee7afc592e4497891c77818a5a3d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Training...', max=20)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 50... Loss: 3.1715... Val Loss: 3.1491\n",
      "Epoch: 1/20... Step: 100... Loss: 3.1736... Val Loss: 3.1425\n",
      "Epoch: 1/20... Step: 150... Loss: 3.1025... Val Loss: 3.0845\n",
      "Epoch: 1/20... Step: 200... Loss: 2.8565... Val Loss: 2.7679\n",
      "Epoch: 1/20... Step: 250... Loss: 2.6564... Val Loss: 2.5843\n",
      "Epoch: 1/20... Step: 300... Loss: 2.5723... Val Loss: 2.4833\n",
      "Epoch: 1/20... Step: 350... Loss: 2.5156... Val Loss: 2.4119\n",
      "Epoch: 1/20... Step: 400... Loss: 2.4298... Val Loss: 2.3523\n",
      "Epoch: 1/20... Step: 450... Loss: 2.3728... Val Loss: 2.3103\n",
      "Epoch: 2/20... Step: 500... Loss: 2.3137... Val Loss: 2.2609\n",
      "Epoch: 2/20... Step: 550... Loss: 2.3018... Val Loss: 2.2200\n",
      "Epoch: 2/20... Step: 600... Loss: 2.2609... Val Loss: 2.1853\n",
      "Epoch: 2/20... Step: 650... Loss: 2.2328... Val Loss: 2.1574\n",
      "Epoch: 2/20... Step: 700... Loss: 2.2373... Val Loss: 2.1281\n",
      "Epoch: 2/20... Step: 750... Loss: 2.2319... Val Loss: 2.1052\n",
      "Epoch: 2/20... Step: 800... Loss: 2.1870... Val Loss: 2.0797\n",
      "Epoch: 2/20... Step: 850... Loss: 2.1646... Val Loss: 2.0568\n",
      "Epoch: 2/20... Step: 900... Loss: 2.1444... Val Loss: 2.0391\n",
      "Epoch: 2/20... Step: 950... Loss: 2.1251... Val Loss: 2.0230\n",
      "Epoch: 3/20... Step: 1000... Loss: 2.1009... Val Loss: 2.0069\n",
      "Epoch: 3/20... Step: 1050... Loss: 2.1453... Val Loss: 1.9898\n",
      "Epoch: 3/20... Step: 1100... Loss: 2.0859... Val Loss: 1.9729\n",
      "Epoch: 3/20... Step: 1150... Loss: 2.0821... Val Loss: 1.9567\n",
      "Epoch: 3/20... Step: 1200... Loss: 2.0546... Val Loss: 1.9445\n",
      "Epoch: 3/20... Step: 1250... Loss: 2.0763... Val Loss: 1.9318\n",
      "Epoch: 3/20... Step: 1300... Loss: 2.0061... Val Loss: 1.9195\n",
      "Epoch: 3/20... Step: 1350... Loss: 1.9985... Val Loss: 1.9092\n",
      "Epoch: 3/20... Step: 1400... Loss: 2.0290... Val Loss: 1.9000\n",
      "Epoch: 3/20... Step: 1450... Loss: 1.9950... Val Loss: 1.8890\n",
      "Epoch: 4/20... Step: 1500... Loss: 2.0027... Val Loss: 1.8772\n",
      "Epoch: 4/20... Step: 1550... Loss: 1.9635... Val Loss: 1.8690\n",
      "Epoch: 4/20... Step: 1600... Loss: 2.0041... Val Loss: 1.8589\n",
      "Epoch: 4/20... Step: 1650... Loss: 1.9970... Val Loss: 1.8506\n",
      "Epoch: 4/20... Step: 1700... Loss: 1.9574... Val Loss: 1.8419\n",
      "Epoch: 4/20... Step: 1750... Loss: 1.9497... Val Loss: 1.8339\n",
      "Epoch: 4/20... Step: 1800... Loss: 1.9440... Val Loss: 1.8244\n",
      "Epoch: 4/20... Step: 1850... Loss: 1.9433... Val Loss: 1.8172\n",
      "Epoch: 4/20... Step: 1900... Loss: 1.9773... Val Loss: 1.8094\n",
      "Epoch: 4/20... Step: 1950... Loss: 1.9011... Val Loss: 1.8054\n",
      "Epoch: 5/20... Step: 2000... Loss: 1.9349... Val Loss: 1.7960\n",
      "Epoch: 5/20... Step: 2050... Loss: 1.9057... Val Loss: 1.7896\n",
      "Epoch: 5/20... Step: 2100... Loss: 1.9133... Val Loss: 1.7831\n",
      "Epoch: 5/20... Step: 2150... Loss: 1.9037... Val Loss: 1.7755\n",
      "Epoch: 5/20... Step: 2200... Loss: 1.8657... Val Loss: 1.7718\n",
      "Epoch: 5/20... Step: 2250... Loss: 1.8811... Val Loss: 1.7633\n",
      "Epoch: 5/20... Step: 2300... Loss: 1.9138... Val Loss: 1.7577\n",
      "Epoch: 5/20... Step: 2350... Loss: 1.8632... Val Loss: 1.7528\n",
      "Epoch: 5/20... Step: 2400... Loss: 1.8647... Val Loss: 1.7467\n",
      "Epoch: 5/20... Step: 2450... Loss: 1.8489... Val Loss: 1.7409\n",
      "Epoch: 6/20... Step: 2500... Loss: 1.8711... Val Loss: 1.7359\n",
      "Epoch: 6/20... Step: 2550... Loss: 1.8677... Val Loss: 1.7289\n",
      "Epoch: 6/20... Step: 2600... Loss: 1.8720... Val Loss: 1.7245\n",
      "Epoch: 6/20... Step: 2650... Loss: 1.8379... Val Loss: 1.7211\n",
      "Epoch: 6/20... Step: 2700... Loss: 1.8758... Val Loss: 1.7142\n",
      "Epoch: 6/20... Step: 2750... Loss: 1.8187... Val Loss: 1.7086\n",
      "Epoch: 6/20... Step: 2800... Loss: 1.8101... Val Loss: 1.7043\n",
      "Epoch: 6/20... Step: 2850... Loss: 1.7747... Val Loss: 1.6989\n",
      "Epoch: 6/20... Step: 2900... Loss: 1.8536... Val Loss: 1.6941\n",
      "Epoch: 6/20... Step: 2950... Loss: 1.8172... Val Loss: 1.6886\n",
      "Epoch: 7/20... Step: 3000... Loss: 1.7876... Val Loss: 1.6856\n",
      "Epoch: 7/20... Step: 3050... Loss: 1.8227... Val Loss: 1.6824\n",
      "Epoch: 7/20... Step: 3100... Loss: 1.8206... Val Loss: 1.6749\n",
      "Epoch: 7/20... Step: 3150... Loss: 1.8427... Val Loss: 1.6704\n",
      "Epoch: 7/20... Step: 3200... Loss: 1.7664... Val Loss: 1.6682\n",
      "Epoch: 7/20... Step: 3250... Loss: 1.7884... Val Loss: 1.6626\n",
      "Epoch: 7/20... Step: 3300... Loss: 1.7747... Val Loss: 1.6577\n",
      "Epoch: 7/20... Step: 3350... Loss: 1.7597... Val Loss: 1.6554\n",
      "Epoch: 7/20... Step: 3400... Loss: 1.7708... Val Loss: 1.6534\n",
      "Epoch: 8/20... Step: 3450... Loss: 1.7528... Val Loss: 1.6497\n",
      "Epoch: 8/20... Step: 3500... Loss: 1.7811... Val Loss: 1.6431\n",
      "Epoch: 8/20... Step: 3550... Loss: 1.7501... Val Loss: 1.6399\n",
      "Epoch: 8/20... Step: 3600... Loss: 1.7506... Val Loss: 1.6361\n",
      "Epoch: 8/20... Step: 3650... Loss: 1.7685... Val Loss: 1.6329\n",
      "Epoch: 8/20... Step: 3700... Loss: 1.7572... Val Loss: 1.6312\n",
      "Epoch: 8/20... Step: 3750... Loss: 1.7419... Val Loss: 1.6271\n",
      "Epoch: 8/20... Step: 3800... Loss: 1.7473... Val Loss: 1.6240\n",
      "Epoch: 8/20... Step: 3850... Loss: 1.7416... Val Loss: 1.6191\n",
      "Epoch: 8/20... Step: 3900... Loss: 1.7101... Val Loss: 1.6197\n",
      "Epoch: 9/20... Step: 3950... Loss: 1.7140... Val Loss: 1.6152\n",
      "Epoch: 9/20... Step: 4000... Loss: 1.7671... Val Loss: 1.6135\n",
      "Epoch: 9/20... Step: 4050... Loss: 1.7467... Val Loss: 1.6105\n",
      "Epoch: 9/20... Step: 4100... Loss: 1.7285... Val Loss: 1.6062\n",
      "Epoch: 9/20... Step: 4150... Loss: 1.6979... Val Loss: 1.6026\n",
      "Epoch: 9/20... Step: 4200... Loss: 1.7350... Val Loss: 1.6016\n",
      "Epoch: 9/20... Step: 4250... Loss: 1.7085... Val Loss: 1.5980\n",
      "Epoch: 9/20... Step: 4300... Loss: 1.7223... Val Loss: 1.5955\n",
      "Epoch: 9/20... Step: 4350... Loss: 1.6964... Val Loss: 1.5911\n",
      "Epoch: 9/20... Step: 4400... Loss: 1.7141... Val Loss: 1.5908\n",
      "Epoch: 10/20... Step: 4450... Loss: 1.7067... Val Loss: 1.5878\n",
      "Epoch: 10/20... Step: 4500... Loss: 1.7158... Val Loss: 1.5888\n",
      "Epoch: 10/20... Step: 4550... Loss: 1.7094... Val Loss: 1.5830\n",
      "Epoch: 10/20... Step: 4600... Loss: 1.7313... Val Loss: 1.5797\n",
      "Epoch: 10/20... Step: 4650... Loss: 1.6858... Val Loss: 1.5778\n",
      "Epoch: 10/20... Step: 4700... Loss: 1.6706... Val Loss: 1.5771\n",
      "Epoch: 10/20... Step: 4750... Loss: 1.6629... Val Loss: 1.5741\n",
      "Epoch: 10/20... Step: 4800... Loss: 1.6700... Val Loss: 1.5726\n",
      "Epoch: 10/20... Step: 4850... Loss: 1.6682... Val Loss: 1.5696\n",
      "Epoch: 10/20... Step: 4900... Loss: 1.6593... Val Loss: 1.5681\n",
      "Epoch: 11/20... Step: 4950... Loss: 1.6598... Val Loss: 1.5661\n",
      "Epoch: 11/20... Step: 5000... Loss: 1.6438... Val Loss: 1.5632\n",
      "Epoch: 11/20... Step: 5050... Loss: 1.6998... Val Loss: 1.5623\n",
      "Epoch: 11/20... Step: 5100... Loss: 1.6706... Val Loss: 1.5591\n",
      "Epoch: 11/20... Step: 5150... Loss: 1.6688... Val Loss: 1.5571\n",
      "Epoch: 11/20... Step: 5200... Loss: 1.6547... Val Loss: 1.5553\n",
      "Epoch: 11/20... Step: 5250... Loss: 1.6715... Val Loss: 1.5539\n",
      "Epoch: 11/20... Step: 5300... Loss: 1.6849... Val Loss: 1.5510\n",
      "Epoch: 11/20... Step: 5350... Loss: 1.6376... Val Loss: 1.5511\n",
      "Epoch: 11/20... Step: 5400... Loss: 1.6882... Val Loss: 1.5510\n",
      "Epoch: 12/20... Step: 5450... Loss: 1.6413... Val Loss: 1.5473\n",
      "Epoch: 12/20... Step: 5500... Loss: 1.6618... Val Loss: 1.5447\n",
      "Epoch: 12/20... Step: 5550... Loss: 1.6408... Val Loss: 1.5450\n",
      "Epoch: 12/20... Step: 5600... Loss: 1.6785... Val Loss: 1.5414\n",
      "Epoch: 12/20... Step: 5650... Loss: 1.6327... Val Loss: 1.5413\n",
      "Epoch: 12/20... Step: 5700... Loss: 1.6155... Val Loss: 1.5393\n",
      "Epoch: 12/20... Step: 5750... Loss: 1.6449... Val Loss: 1.5395\n",
      "Epoch: 12/20... Step: 5800... Loss: 1.6378... Val Loss: 1.5349\n",
      "Epoch: 12/20... Step: 5850... Loss: 1.6184... Val Loss: 1.5349\n",
      "Epoch: 12/20... Step: 5900... Loss: 1.6299... Val Loss: 1.5319\n",
      "Epoch: 13/20... Step: 5950... Loss: 1.6467... Val Loss: 1.5309\n",
      "Epoch: 13/20... Step: 6000... Loss: 1.6142... Val Loss: 1.5297\n",
      "Epoch: 13/20... Step: 6050... Loss: 1.6357... Val Loss: 1.5287\n",
      "Epoch: 13/20... Step: 6100... Loss: 1.6167... Val Loss: 1.5269\n",
      "Epoch: 13/20... Step: 6150... Loss: 1.6451... Val Loss: 1.5262\n",
      "Epoch: 13/20... Step: 6200... Loss: 1.6650... Val Loss: 1.5235\n",
      "Epoch: 13/20... Step: 6250... Loss: 1.6215... Val Loss: 1.5227\n",
      "Epoch: 13/20... Step: 6300... Loss: 1.6254... Val Loss: 1.5210\n",
      "Epoch: 13/20... Step: 6350... Loss: 1.6333... Val Loss: 1.5211\n",
      "Epoch: 14/20... Step: 6400... Loss: 1.6103... Val Loss: 1.5216\n",
      "Epoch: 14/20... Step: 6450... Loss: 1.6132... Val Loss: 1.5175\n",
      "Epoch: 14/20... Step: 6500... Loss: 1.6485... Val Loss: 1.5172\n",
      "Epoch: 14/20... Step: 6550... Loss: 1.6514... Val Loss: 1.5175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20... Step: 6600... Loss: 1.6195... Val Loss: 1.5133\n",
      "Epoch: 14/20... Step: 6650... Loss: 1.6221... Val Loss: 1.5134\n",
      "Epoch: 14/20... Step: 6700... Loss: 1.6122... Val Loss: 1.5128\n",
      "Epoch: 14/20... Step: 6750... Loss: 1.5832... Val Loss: 1.5103\n",
      "Epoch: 14/20... Step: 6800... Loss: 1.5896... Val Loss: 1.5089\n",
      "Epoch: 14/20... Step: 6850... Loss: 1.6062... Val Loss: 1.5091\n",
      "Epoch: 15/20... Step: 6900... Loss: 1.6082... Val Loss: 1.5061\n",
      "Epoch: 15/20... Step: 6950... Loss: 1.6088... Val Loss: 1.5062\n",
      "Epoch: 15/20... Step: 7000... Loss: 1.6042... Val Loss: 1.5058\n",
      "Epoch: 15/20... Step: 7050... Loss: 1.6055... Val Loss: 1.5040\n",
      "Epoch: 15/20... Step: 7100... Loss: 1.5986... Val Loss: 1.5035\n",
      "Epoch: 15/20... Step: 7150... Loss: 1.6006... Val Loss: 1.5030\n",
      "Epoch: 15/20... Step: 7200... Loss: 1.6108... Val Loss: 1.5039\n",
      "Epoch: 15/20... Step: 7250... Loss: 1.5846... Val Loss: 1.5016\n",
      "Epoch: 15/20... Step: 7300... Loss: 1.5708... Val Loss: 1.4989\n",
      "Epoch: 15/20... Step: 7350... Loss: 1.5641... Val Loss: 1.4990\n",
      "Epoch: 16/20... Step: 7400... Loss: 1.6308... Val Loss: 1.4977\n",
      "Epoch: 16/20... Step: 7450... Loss: 1.6094... Val Loss: 1.4980\n",
      "Epoch: 16/20... Step: 7500... Loss: 1.6244... Val Loss: 1.4952\n",
      "Epoch: 16/20... Step: 7550... Loss: 1.5922... Val Loss: 1.4956\n",
      "Epoch: 16/20... Step: 7600... Loss: 1.5861... Val Loss: 1.4946\n",
      "Epoch: 16/20... Step: 7650... Loss: 1.5944... Val Loss: 1.4938\n",
      "Epoch: 16/20... Step: 7700... Loss: 1.5814... Val Loss: 1.4927\n",
      "Epoch: 16/20... Step: 7750... Loss: 1.5916... Val Loss: 1.4913\n",
      "Epoch: 16/20... Step: 7800... Loss: 1.5842... Val Loss: 1.4909\n",
      "Epoch: 16/20... Step: 7850... Loss: 1.5734... Val Loss: 1.4906\n",
      "Epoch: 17/20... Step: 7900... Loss: 1.5658... Val Loss: 1.4912\n",
      "Epoch: 17/20... Step: 7950... Loss: 1.5824... Val Loss: 1.4875\n",
      "Epoch: 17/20... Step: 8000... Loss: 1.5842... Val Loss: 1.4899\n",
      "Epoch: 17/20... Step: 8050... Loss: 1.5853... Val Loss: 1.4869\n",
      "Epoch: 17/20... Step: 8100... Loss: 1.5602... Val Loss: 1.4868\n",
      "Epoch: 17/20... Step: 8150... Loss: 1.5771... Val Loss: 1.4847\n",
      "Epoch: 17/20... Step: 8200... Loss: 1.5377... Val Loss: 1.4861\n",
      "Epoch: 17/20... Step: 8250... Loss: 1.5743... Val Loss: 1.4839\n",
      "Epoch: 17/20... Step: 8300... Loss: 1.5555... Val Loss: 1.4825\n",
      "Epoch: 17/20... Step: 8350... Loss: 1.5654... Val Loss: 1.4813\n",
      "Epoch: 18/20... Step: 8400... Loss: 1.5644... Val Loss: 1.4817\n",
      "Epoch: 18/20... Step: 8450... Loss: 1.5746... Val Loss: 1.4810\n",
      "Epoch: 18/20... Step: 8500... Loss: 1.6001... Val Loss: 1.4805\n",
      "Epoch: 18/20... Step: 8550... Loss: 1.5805... Val Loss: 1.4801\n",
      "Epoch: 18/20... Step: 8600... Loss: 1.5345... Val Loss: 1.4788\n",
      "Epoch: 18/20... Step: 8650... Loss: 1.5541... Val Loss: 1.4779\n",
      "Epoch: 18/20... Step: 8700... Loss: 1.5721... Val Loss: 1.4787\n",
      "Epoch: 18/20... Step: 8750... Loss: 1.5475... Val Loss: 1.4770\n",
      "Epoch: 18/20... Step: 8800... Loss: 1.5611... Val Loss: 1.4767\n",
      "Epoch: 18/20... Step: 8850... Loss: 1.5729... Val Loss: 1.4762\n",
      "Epoch: 19/20... Step: 8900... Loss: 1.5775... Val Loss: 1.4749\n",
      "Epoch: 19/20... Step: 8950... Loss: 1.5816... Val Loss: 1.4747\n",
      "Epoch: 19/20... Step: 9000... Loss: 1.5640... Val Loss: 1.4740\n",
      "Epoch: 19/20... Step: 9050... Loss: 1.5422... Val Loss: 1.4745\n",
      "Epoch: 19/20... Step: 9100... Loss: 1.5771... Val Loss: 1.4743\n",
      "Epoch: 19/20... Step: 9150... Loss: 1.5799... Val Loss: 1.4710\n",
      "Epoch: 19/20... Step: 9200... Loss: 1.5506... Val Loss: 1.4715\n",
      "Epoch: 19/20... Step: 9250... Loss: 1.5409... Val Loss: 1.4715\n",
      "Epoch: 19/20... Step: 9300... Loss: 1.6061... Val Loss: 1.4714\n",
      "Epoch: 20/20... Step: 9350... Loss: 1.5650... Val Loss: 1.4701\n",
      "Epoch: 20/20... Step: 9400... Loss: 1.5629... Val Loss: 1.4706\n",
      "Epoch: 20/20... Step: 9450... Loss: 1.6032... Val Loss: 1.4694\n",
      "Epoch: 20/20... Step: 9500... Loss: 1.5816... Val Loss: 1.4678\n",
      "Epoch: 20/20... Step: 9550... Loss: 1.5699... Val Loss: 1.4687\n",
      "Epoch: 20/20... Step: 9600... Loss: 1.5703... Val Loss: 1.4671\n",
      "Epoch: 20/20... Step: 9650... Loss: 1.5446... Val Loss: 1.4663\n",
      "Epoch: 20/20... Step: 9700... Loss: 1.5268... Val Loss: 1.4669\n",
      "Epoch: 20/20... Step: 9750... Loss: 1.5669... Val Loss: 1.4653\n",
      "Epoch: 20/20... Step: 9800... Loss: 1.5595... Val Loss: 1.4662\n",
      "AAAAAAS ALERITHS ATEANT\n",
      "ANd THER THE STRELES WHEN AST STAN TOUTHES\n",
      "SHALL THING WITH TO LANG WHERES WO HOSE\n",
      "SONE\n",
      "IN ING SEELS SOMLENDIS\n",
      "THE CONSERT THER SHINTS AND SHANLIN THE ANT STALL ATER\n",
      "WALL IN THING, THE\n",
      "BLUTLING I SEENES WELLOL WERE SHALL THULL THE LACLO LATS ALDE TRISS SHILL THER ASS ATTESS SO ANS WHELL AS TOULSS WENT IN THE SHILL ASTUSS WHALLIT WHAT LACES SHE THE ASKES THE LOTH AS THE STRE THER IS THAT WOULD ALD AND I SHERE SOR ING THALTH TO LASE AN WIST\n",
      "SO TO LARDEN I TO IS THE TELINS ANE SOUTH IN\n",
      "THE LETINT STARE TOTOUTILINGSE WOR SORSS SINL ANDILES ALANS WALL IS THE SELLARS THAT THE WING SIND THE WHITE IS THE ANTHINE SHELL ASTON\n",
      "AND\n",
      "THE TOUTERS STON THE\n",
      "WASTIRSS\n",
      "SOM WHE LY ANDER IN A ALL ATEANGS OR WELISSEST AND ALL STANDE SOUT IN THE WHERE WALL WHEN AND THE SONGE\n",
      "AT LES THEN THE TRESERING THE SEELS, SAR IN IN THE\n",
      "ASKER WELE AND AND WHEN IT\n",
      "THE\n",
      "TENT IN IT SELL ILS YILL AMERING\n",
      "THE LOGH AT SHASKE ANITH IT\n",
      "THEST I SELL THOM\n",
      "SANE INE SHOUT THEN IS\n",
      "THE WILK TERE SIS SHATHE\n",
      "WAS SHE\n"
     ]
    }
   ],
   "source": [
    "# Define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = crnn.CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)\n",
    "\n",
    "# Declaring the hyperparameters\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "crnn.train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=250)\n",
    "\n",
    "# Saving the model\n",
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              \n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)\n",
    "    \n",
    "# Generating new text\n",
    "print(crnn.sample(net, 1000, prime='A', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "The search is that\n",
      "So walked and stay on the sense of the step, to the strike\n",
      "Where a silver station are the flower of the battles.\n",
      "A poor book of the troulds of an expectent tongue\n",
      "And starts alone the brown bare sound and song.\n",
      "To his beats through the train to be too look\n",
      "Them through the song to seem on the black branches and the down,\n",
      "Through the surface soul with a back and as they shall say an \n",
      "10\n",
      "Ther mayst, the frumeration wurl\n",
      "Shit of the mountain wonder brief, waiting to\n",
      "the softers\n",
      "We are a steam as wearons back\n",
      "Of this house for the come to still feed.\n",
      "No place to mention me a carp\n",
      "In the door\n",
      "In the people the temple\n",
      "I continue a lawnchear\n",
      "As the double sea between turned song,\n",
      "And saying the words that watch the flowers\n",
      "Implaces by a cloud of concisions,\n",
      "And sailt like a friend being to\n",
      "15\n",
      "Then banks:\n",
      "Certainly of the grible couldn,\n",
      "And we lived in light, before my great\n",
      "Players, to muldiculate skins,\n",
      "To die, and my beh aland in any wend,\n",
      "Others can not spare aloft in her cause,\n",
      "A past presse to leap and take;\n",
      "The things he grassed human gardens,\n",
      "The soldiers were neighbor, wait\n",
      "So fasiries me on whatever the\n",
      "And of my dream and set inferted—\n",
      "And that now its fling slipless us,\n",
      "As futur\n",
      "20\n",
      "The bodies’ tizets\n",
      "She lives flowers up into trees; trod,\n",
      "Oded, stowed behind your ruggle, have grite.\n",
      "Night, though he could transform and swell,\n",
      "So I know, pulled legs my grandeum black than\n",
      "The other waters people or the same\n",
      "Wood, her rise as the god was steady graceful crase\n",
      "And wide today the pain of the chench\n",
      "Of the sky swars like pure wheel;\n",
      "With each thousand bloody shurber pressed\n",
      "Nor shall\n",
      "25\n",
      "The expressive I was only\n",
      "Proparal greetness thus or\n",
      "Can\n",
      "the fields impalling the murdered tomelides and small\n",
      "Else to be up—the heart, how notice?\n",
      "If you are to discovered the lewdless face,\n",
      "Almost now a brand and rubbed the little day of the hand\n",
      "Is threat with a snudder tracks wamming on the wind:\n",
      "With the white is I hey to sitch—\n",
      "Her, for delands in that passion of went\n",
      "As an orbits of feathering \n",
      "30\n",
      "The pocket\n",
      "As he left the settled them on love pleasing\n",
      "at the dark eyes.Who think the weeks that found\n",
      "him we came down. In a rice,\n",
      "pile I ruthe a skinned\n",
      "coffee brick tenincular life\n",
      "of chemically.\n",
      "Streaking in brandippy rit in laws starts,\n",
      "at the task of first planed—a bird fathing\n",
      "out of will open room’s time\n",
      "gone, the mad leaves: elsewhere donne\n",
      "not vosbit where my harms had\n",
      "a near thensweedly tw\n",
      "35\n",
      "The sad bridges.\n",
      "It is Gen, like this palish of San.\n",
      "It marked it. Second the falling\n",
      "Side offer a few demix's place\n",
      "And ciffer hadded me with much,\n",
      "To her crating out her last.\n",
      "Half mines like bliss towerolls her,\n",
      "Fur dampeder! Trees will not accost\n",
      "Emerge even very thigger of his rasks.\n",
      "Was display or dispose each\n",
      "Ix and the sweet rise,\n",
      "Photogrophed for a sheet.’\n",
      "He sits’s this history, their faich \n",
      "40\n",
      "The was it woods\n",
      "Through the white Black is decare,\n",
      "And the oragic gray queen\n",
      "Of lueking praise\n",
      "More beauty to shout me to watch home\n",
      "Akong her either kind to the speech!\n",
      "Whip demboderness sutto this\n",
      "who more the made your blue perfect\n",
      "Latce can welt love\n",
      "Track the heavy body of launce\n",
      "What I fought her\n",
      "How half things\n",
      "They reach.\n",
      "You white step hungered,\n",
      "Was no not’s lamps so\n",
      "Force, explaining the bl\n",
      "45\n",
      "The’s staging in Gadey,\n",
      "Revolution is donning relectly to a short,\n",
      "I put the fair, while he wasn’t begun home;\n",
      "From answer in every old did fait've say.I’ve eye, fun the dapt that extout the street the same\n",
      "of as which is going the note\n",
      "of the strain that made you; still weak with the flower. Naziness\n",
      "ebettens out back with some a rigin tomnial\n",
      "povorate on the shaft; blue in the delight on\n",
      "their someh\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 50, 5):\n",
    "    print(i)\n",
    "    print(crnn.sample(net, 400, prime=\"The\", top_k=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127. SOUTH, IT SHALL BE THE TIME WHEN NEGROES LEAVE THE\n",
      "283. SUMMER\n",
      "1423. “M-A-R-R-Y M-E!”\n",
      "1980. (A\n",
      "2406. INFOMERCIALS HAVE STARTED AND I KIND OF WANT TO DIE\n",
      "3097. V\n",
      "7600. F\n",
      "9098. O\n"
     ]
    }
   ],
   "source": [
    "num_poems = 10215;\n",
    "\n",
    "text = \"\"\n",
    "for i in range(1,num_poems+1):\n",
    "    text = json.load(open(\"./poems/\"+ str(i) + \".json\"))['text']\n",
    "    if(len(text)>2 and text[2].isupper()):\n",
    "        print(str(i)+ \". \" + text[2])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
